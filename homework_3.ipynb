{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IF2A682ZP3qN"
   },
   "source": [
    "## Generative Adversarial Networks on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will create and train a GAN to generate images of digits that mimic those in the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5EpzTB6P3qO"
   },
   "source": [
    "### Evaluation metric: Inception Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4Gb-AhYP3qO"
   },
   "source": [
    "Rather than just eye-balling whether GAN samples look good or not, researchers have come up with mulitple objective metrics for determining the quality and the diversity of GAN outputs. We will use one of the metrics called the *Inception Score*.\n",
    "\n",
    "Calculating the Inception Score involves running a pretrained neural network. This is where the name is from: the authors who proposed this metric used a pretrained [Inception Network](https://arxiv.org/pdf/1409.4842.pdf) from Tensorflow in their [paper](https://proceedings.neurips.cc/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf). Since we will be using the MNIST dataset in this assignment, we provide a simpler neural work pretrained on MNIST as the scoring model.\n",
    "\n",
    "The idea behind the Inception Score is simple: a good GAN should generate *meaningful* and *diverse* samples. For MNIST, a specific sample is \"meaningful\" if it looks like any of the 10 digits. When we take a good digit classifier and run it on this sample, it should assign high probability to one of the 10 classes and low probability to the others. In information theory terms, this means the predicted label distribution $p(y|x)$ for any specific sample $x$ should have high entropy. On the other hand, if the generated samples are diverse, they should be able to cover all 10 classes when we generate a large enough set of samples. This means that the \"average\" label distribution $p(y) = \\int p(y|x=G(z)) \\mathrm{d}z$ should have low entropy. The Inception Score is define by $\\exp (\\mathbb{E}_x \\mathrm{KL}(p(y|x) || p(y)))$, where $\\mathrm{KL}(P||Q)$ is the K-L divergence, which is often used to measure how probability distribution $P$ is different from distribution $Q$. Intuitively, if the generated samples are good, $p(y|x)$ should be different from $p(y)$, since one should have high entropy while the other should have low entropy.\n",
    "\n",
    "Don't be too worried if you don't fully get how the score is defined and calculated. Just remember that in this assignment, we want our GAN to have a high Inception Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U51Pv7gnP3qO"
   },
   "outputs": [],
   "source": [
    "# Pretrained model used to evaluation the inception score.\n",
    "class ScoringModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "def inception_score_mnist(\n",
    "    imgs,\n",
    "    model_path='weights/mnist.ckpt',\n",
    "    batch_size=32,\n",
    "    num_splits=10,\n",
    "):\n",
    "    \"\"\"Computes the inception score of `imgs`.\n",
    "    \n",
    "    Args:\n",
    "    - imgs: Array of size (number of data points, 1, 28, 28)\n",
    "    - batch_size: Batch size for feeding data into the pretrained MNIST model.\n",
    "    - num_splits: Number of splits. We split the samples into multiple subsets\n",
    "        and calculate the scores on each of them. Their mean is used as the\n",
    "        final score.\n",
    "    \"\"\"\n",
    "    # Verify that input arguments have the correct formats.\n",
    "    assert type(imgs) == np.ndarray\n",
    "    assert imgs.shape[1:] == (1, 28, 28)\n",
    "    assert batch_size > 0\n",
    "    assert len(imgs) > batch_size\n",
    "    \n",
    "    # Choose device to be used.\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Preprocess input.\n",
    "    imgs = copy.copy(imgs)\n",
    "    imgs = (imgs - 0.1307) / 0.3081\n",
    "    \n",
    "    # Set up dataloader.\n",
    "    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "\n",
    "    # Load pretrained scoring model.\n",
    "    model = ScoringModel()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get predictions.\n",
    "    preds = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        preds.append(probs)\n",
    "    preds = np.concatenate(preds)\n",
    "\n",
    "    # Compute the mean KL divergence.\n",
    "    split_scores = []\n",
    "\n",
    "    for i in range(num_splits):\n",
    "        n = len(imgs) // num_splits\n",
    "        split = preds[i*n:(i+1)*n, :]\n",
    "        py = np.mean(split, axis=0)\n",
    "        scores = []\n",
    "        for i in range(split.shape[0]):\n",
    "            pyx = split[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "    \n",
    "    return np.mean(split_scores), np.std(split_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsyZJXAOP3qO"
   },
   "source": [
    "Now, let's try to calculate the Inception score on the actual MNIST dataset.\n",
    "\n",
    "Make sure that the provided file `mnist.ckpt` is under `./weights`. Alternatively, you can specify its path via the `model_path` argument of `inception_score_mnist()`. If using Google Colab, click `View > Table of Contents > Files` and then upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JlXCEGNBP3qO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (500, 1, 28, 28)\n",
      "Inception Score: mean=8.933, std=0.498\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data', train=False, download=True, transform=transform),\n",
    "    batch_size=500, shuffle=True)\n",
    "\n",
    "x, _ = next(iter(train_loader))\n",
    "x = x.cpu().data.numpy()\n",
    "x = x.reshape((-1,1,28,28))\n",
    "print('Shape of data:',x.shape)\n",
    "mean, std = inception_score_mnist(x)\n",
    "print(f'Inception Score: mean={mean:.3f}, std={std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPpHfMZ_P3qO"
   },
   "source": [
    "The score for the real MNIST dataset should be above 8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjoMze3uP3qO"
   },
   "source": [
    "### Generating MNIST images (100 points)\n",
    "\n",
    "As you did with the Gaussian distribution example in the weekly notebook, define and train a GAN to generate images that mimic those in the MNIST dataset.\n",
    "\n",
    "#### Deliverables\n",
    "\n",
    "- After training your model, generate at least 1500 samples using the trained generator, and evaluate your model by calculating the Inception score on the generated samples.\n",
    "- Pick a few generated samples and visualize them.\n",
    "- Plot the training losses for the discriminator and the generator.\n",
    "\n",
    "Given the limited computational resources, you will want to achieve an Inception score of 1.5 or greater for full credits. A score of 1.5 won't yield great images. For nice looking images, you'll need an Inception score of around 6.0, but it is not needed for full credits.\n",
    "\n",
    "#### Model Submission\n",
    "\n",
    "For more complicated architectures, if your model takes a long time to train, you will need to save the model and write a code snippet that loads it such that the code runs with no errors and we can grade it easily. In this case, set `epochs = 0` and include the saved model in your submission (or a Google drive share link if its too large).\n",
    "\n",
    "#### Tips\n",
    "\n",
    "- It will be easier to get better results with a convolutional GAN. You may find this [tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) on [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) helpful. The generators of DCGANs make use of transposed convolutions (`nn.ConvTranspose2d` in PyTorch) to map features to larger sizes. This [article](https://d2l.ai/chapter_computer-vision/transposed-conv.html) does a good job illustrating how they work.\n",
    "- Feel free to try different architectures, layers, optimizers, training schemes and other hyperparameters. Particularly, if training with one type of optimizer is slow or unstable, give other types of optimizers a try.\n",
    "\n",
    "There are plenty of online resources about GAN that you can reference for inspiration. But do not plagiarize. Please write your own custom networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " GAN_lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
